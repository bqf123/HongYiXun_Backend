# æ–°çˆ¬è™«å¼€å‘æ¨¡æ¿

æœ¬ç›®å½•æä¾›äº†å¼€å‘æ–°çˆ¬è™«çš„å®Œæ•´æ¨¡æ¿ä»£ç ,å¸®åŠ©å›¢é˜Ÿæˆå‘˜å¿«é€Ÿå¼€å§‹å¼€å‘ã€‚

## ğŸ“ æ¨¡æ¿æ–‡ä»¶è¯´æ˜

```
templates/
â”œâ”€â”€ crawler_template.py          # çˆ¬è™«ç±»æ¨¡æ¿
â”œâ”€â”€ api_template.py              # APIè·¯ç”±æ¨¡æ¿
â”œâ”€â”€ model_template.py            # æ•°æ®æ¨¡å‹æ¨¡æ¿
â””â”€â”€ README.md                    # æœ¬æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1: å¤åˆ¶æ¨¡æ¿æ–‡ä»¶

```bash
# å‡è®¾ä½ è¦å¼€å‘ "åä¸ºæ–°é—»" çˆ¬è™«

# 1. å¤åˆ¶çˆ¬è™«æ¨¡æ¿
cp templates/crawler_template.py services/huawei_news_crawler.py

# 2. å¤åˆ¶APIæ¨¡æ¿
cp templates/api_template.py api/huawei_news.py

# 3. å¤åˆ¶æ¨¡å‹æ¨¡æ¿(å¯é€‰)
cp templates/model_template.py models/huawei_news.py
```

### æ­¥éª¤2: å…¨å±€æ›¿æ¢å ä½ç¬¦

åœ¨å¤åˆ¶çš„æ–‡ä»¶ä¸­,ä½¿ç”¨ç¼–è¾‘å™¨çš„"æŸ¥æ‰¾æ›¿æ¢"åŠŸèƒ½:

| å ä½ç¬¦ | æ›¿æ¢ä¸º | ç¤ºä¾‹ |
|--------|--------|------|
| `YourDataSource` | ä½ çš„æ•°æ®æºåç§°(é©¼å³°) | `HuaweiNews` |
| `your_datasource` | ä½ çš„æ•°æ®æºåç§°(ä¸‹åˆ’çº¿) | `huawei_news` |
| `Your Data Source` | ä½ çš„æ•°æ®æºåç§°(ä¸­æ–‡) | `åä¸ºæ–°é—»` |
| `https://example.com` | ç›®æ ‡ç½‘ç«™URL | `https://www.huawei.com/cn/news` |

**VS Code å¿«æ·é”®**: `Ctrl+H` (Windows) / `Cmd+H` (Mac)

### æ­¥éª¤3: å®ç°ä¸šåŠ¡é€»è¾‘

æ ¹æ®ä½ çš„ç›®æ ‡ç½‘ç«™,å¡«å……ä»¥ä¸‹æ–¹æ³•:

```python
# services/huawei_news_crawler.py

def fetch_list(self, page: int = 1) -> List[Dict]:
    """å®ç°:å¦‚ä½•è·å–åˆ—è¡¨é¡µæ•°æ®"""
    pass

def fetch_detail(self, detail_url: str) -> Optional[Dict]:
    """å®ç°:å¦‚ä½•è·å–è¯¦æƒ…é¡µæ•°æ®"""
    pass

def save_to_database(self, data: List[Dict]) -> int:
    """å®ç°:å¦‚ä½•ä¿å­˜åˆ°æ•°æ®åº“"""
    pass
```

### æ­¥éª¤4: æµ‹è¯•çˆ¬è™«

```bash
# è¿è¡Œæµ‹è¯•
python services/huawei_news_crawler.py

# é¢„æœŸè¾“å‡º
# çˆ¬å–å®Œæˆ: {'total': 20, 'saved': 20, 'status': 'success'}
```

### æ­¥éª¤5: æ³¨å†ŒAPIè·¯ç”±

åœ¨ `main.py` ä¸­æ³¨å†Œä½ çš„API:

```python
# main.py
from api import news, banner, huawei_news  # æ·»åŠ ä½ çš„å¯¼å…¥

app = FastAPI(title="NowInOpenHarmony API")

# æ³¨å†Œè·¯ç”±
app.include_router(news.router)
app.include_router(banner.router)
app.include_router(huawei_news.router)  # æ·»åŠ è¿™è¡Œ
```

### æ­¥éª¤6: æµ‹è¯•API

```bash
# å¯åŠ¨æœåŠ¡
python run.py

# è®¿é—® API æ–‡æ¡£
# http://localhost:8001/docs

# æµ‹è¯•ä½ çš„API
curl http://localhost:8001/api/huawei-news/
curl -X POST http://localhost:8001/api/huawei-news/crawl
```

## ğŸ“ å¼€å‘æŒ‡å—

### æ•°æ®åº“è¡¨è®¾è®¡å»ºè®®

```python
# è¡¨å: <æ•°æ®æº>_<æ•°æ®ç±»å‹>
# ä¾‹å¦‚: huawei_news_articles

CREATE TABLE IF NOT EXISTS huawei_news_articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT NOT NULL,                    -- æ ‡é¢˜(å¿…éœ€)
    url TEXT UNIQUE NOT NULL,               -- URL(å¿…éœ€,å”¯ä¸€)
    content TEXT,                           -- å†…å®¹
    summary TEXT,                           -- æ‘˜è¦
    author TEXT,                            -- ä½œè€…
    category TEXT,                          -- åˆ†ç±»
    tags TEXT,                              -- æ ‡ç­¾(JSONæ•°ç»„å­—ç¬¦ä¸²)
    cover_image TEXT,                       -- å°é¢å›¾ç‰‡URL
    view_count INTEGER DEFAULT 0,          -- æµè§ˆé‡
    publish_time DATETIME,                  -- å‘å¸ƒæ—¶é—´
    crawl_time DATETIME DEFAULT CURRENT_TIMESTAMP,  -- çˆ¬å–æ—¶é—´
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- ç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_url ON huawei_news_articles(url);
CREATE INDEX IF NOT EXISTS idx_publish_time ON huawei_news_articles(publish_time DESC);
CREATE INDEX IF NOT EXISTS idx_category ON huawei_news_articles(category);
```

### çˆ¬è™«æœ€ä½³å®è·µ

#### 1. éµå®ˆ robots.txt

```python
# åœ¨çˆ¬è™«å¼€å§‹å‰æ£€æŸ¥
from urllib.robotparser import RobotFileParser

rp = RobotFileParser()
rp.set_url("https://example.com/robots.txt")
rp.read()

if not rp.can_fetch("*", "https://example.com/page"):
    logger.warning("è¯¥é¡µé¢ä¸å…è®¸çˆ¬å–")
    return
```

#### 2. æ·»åŠ è¯·æ±‚å»¶è¿Ÿ

```python
import time
import random

# åœ¨è¯·æ±‚ä¹‹é—´æ·»åŠ éšæœºå»¶è¿Ÿ
time.sleep(random.uniform(1, 3))  # 1-3ç§’éšæœºå»¶è¿Ÿ
```

#### 3. è®¾ç½®åˆç†çš„ User-Agent

```python
self.session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
})
```

#### 4. å¼‚å¸¸å¤„ç†

```python
def fetch_with_retry(self, url: str, max_retries: int = 3) -> Optional[str]:
    """å¸¦é‡è¯•çš„è¯·æ±‚"""
    for i in range(max_retries):
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            logger.warning(f"è¯·æ±‚å¤±è´¥(ç¬¬{i+1}æ¬¡): {e}")
            if i < max_retries - 1:
                time.sleep(2 ** i)  # æŒ‡æ•°é€€é¿
            else:
                logger.error(f"è¯·æ±‚æœ€ç»ˆå¤±è´¥: {url}")
                return None
```

#### 5. æ•°æ®å»é‡

```python
def is_exists(self, url: str) -> bool:
    """æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨"""
    from core.database import execute_query

    result = execute_query(
        "SELECT COUNT(*) as count FROM your_table WHERE url = ?",
        (url,)
    )
    return result[0]['count'] > 0 if result else False
```

### API è®¾è®¡å»ºè®®

#### 1. ç»Ÿä¸€çš„å“åº”æ ¼å¼

```python
# æˆåŠŸå“åº”
{
    "code": 200,
    "message": "success",
    "data": {
        "total": 100,
        "page": 1,
        "page_size": 20,
        "items": [...]
    }
}

# é”™è¯¯å“åº”
{
    "code": 500,
    "message": "Internal server error",
    "data": None
}
```

#### 2. åˆ†é¡µå‚æ•°éªŒè¯

```python
@router.get("/")
async def get_list(
    page: int = Query(1, ge=1, le=1000, description="é¡µç ,1-1000"),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡,1-100"),
):
    # è®¡ç®—åç§»é‡
    offset = (page - 1) * page_size
    ...
```

#### 3. æœç´¢åŠŸèƒ½

```python
@router.get("/search")
async def search(
    keyword: str = Query(..., min_length=1, max_length=100),
    category: Optional[str] = Query(None),
    start_date: Optional[str] = Query(None),
    end_date: Optional[str] = Query(None),
):
    """
    æœç´¢åŠŸèƒ½
    - keyword: æœç´¢å…³é”®è¯(å¿…éœ€)
    - category: åˆ†ç±»ç­›é€‰(å¯é€‰)
    - start_date: å¼€å§‹æ—¥æœŸ(å¯é€‰,æ ¼å¼:YYYY-MM-DD)
    - end_date: ç»“æŸæ—¥æœŸ(å¯é€‰)
    """
    pass
```

## ğŸ¯ å®Œæ•´ç¤ºä¾‹

å‡è®¾ä½ è¦çˆ¬å– "åä¸ºå¼€å‘è€…æ–°é—»":

### 1. çˆ¬è™«æ–‡ä»¶ (services/huawei_dev_news_crawler.py)

```python
"""
åä¸ºå¼€å‘è€…æ–°é—»çˆ¬è™«
ç›®æ ‡ç½‘ç«™: https://developer.huawei.com/consumer/cn/news/
"""

import logging
from typing import List, Dict, Optional
from datetime import datetime
import time
import random

import requests
from bs4 import BeautifulSoup

from core.database import execute_query, execute_update

logger = logging.getLogger(__name__)


class HuaweiDevNewsCrawler:
    """åä¸ºå¼€å‘è€…æ–°é—»çˆ¬è™«"""

    BASE_URL = "https://developer.huawei.com/consumer/cn/news"

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self._init_database()

    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS huawei_dev_news (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            url TEXT UNIQUE NOT NULL,
            summary TEXT,
            cover_image TEXT,
            publish_time TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
        execute_update(create_table_sql)
        execute_update("CREATE INDEX IF NOT EXISTS idx_url ON huawei_dev_news(url)")

    def fetch_list(self, page: int = 1) -> List[Dict]:
        """è·å–æ–°é—»åˆ—è¡¨"""
        try:
            url = f"{self.BASE_URL}?page={page}"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            news_items = soup.select('.news-item')  # æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´

            results = []
            for item in news_items:
                try:
                    title = item.select_one('.title').get_text(strip=True)
                    link = item.select_one('a')['href']
                    summary = item.select_one('.summary').get_text(strip=True)

                    results.append({
                        'title': title,
                        'url': link if link.startswith('http') else self.BASE_URL + link,
                        'summary': summary,
                    })
                except Exception as e:
                    logger.error(f"è§£ææ–°é—»é¡¹å¤±è´¥: {e}")

            logger.info(f"è·å–åˆ° {len(results)} æ¡æ–°é—»")
            return results

        except Exception as e:
            logger.error(f"è·å–æ–°é—»åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def save_to_database(self, data: List[Dict]) -> int:
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        count = 0
        for item in data:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                exists = execute_query(
                    "SELECT COUNT(*) as cnt FROM huawei_dev_news WHERE url = ?",
                    (item['url'],)
                )

                if exists and exists[0]['cnt'] > 0:
                    continue

                # æ’å…¥æ•°æ®
                execute_update(
                    """
                    INSERT INTO huawei_dev_news (title, url, summary, cover_image, publish_time)
                    VALUES (?, ?, ?, ?, ?)
                    """,
                    (
                        item['title'],
                        item['url'],
                        item.get('summary', ''),
                        item.get('cover_image', ''),
                        item.get('publish_time', ''),
                    )
                )
                count += 1

            except Exception as e:
                logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")

        logger.info(f"æˆåŠŸä¿å­˜ {count} æ¡æ–°æ•°æ®")
        return count


def crawl_all() -> Dict:
    """æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""
    crawler = HuaweiDevNewsCrawler()

    all_data = []
    for page in range(1, 6):
        data = crawler.fetch_list(page)
        all_data.extend(data)
        time.sleep(random.uniform(1, 2))  # éšæœºå»¶è¿Ÿ

    saved = crawler.save_to_database(all_data)

    return {
        'total': len(all_data),
        'saved': saved,
        'status': 'success'
    }


if __name__ == "__main__":
    result = crawl_all()
    print(f"çˆ¬å–å®Œæˆ: {result}")
```

### 2. APIæ–‡ä»¶ (api/huawei_dev_news.py)

```python
"""åä¸ºå¼€å‘è€…æ–°é—» API"""

from typing import Optional
from fastapi import APIRouter, Query, HTTPException

from core.database import execute_query
from services.huawei_dev_news_crawler import crawl_all

router = APIRouter(prefix="/api/huawei-dev-news", tags=["åä¸ºå¼€å‘è€…æ–°é—»"])


@router.get("/")
async def get_news_list(
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
):
    """è·å–åä¸ºå¼€å‘è€…æ–°é—»åˆ—è¡¨"""
    try:
        offset = (page - 1) * page_size

        # æŸ¥è¯¢æ€»æ•°
        total_result = execute_query("SELECT COUNT(*) as total FROM huawei_dev_news")
        total = total_result[0]['total'] if total_result else 0

        # æŸ¥è¯¢æ•°æ®
        items = execute_query(
            """
            SELECT id, title, url, summary, cover_image, publish_time, created_at
            FROM huawei_dev_news
            ORDER BY created_at DESC
            LIMIT ? OFFSET ?
            """,
            (page_size, offset)
        )

        return {
            "code": 200,
            "message": "success",
            "data": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "items": items or []
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/crawl")
async def trigger_crawl():
    """æ‰‹åŠ¨è§¦å‘çˆ¬å–"""
    try:
        result = crawl_all()
        return {
            "code": 200,
            "message": "çˆ¬å–å®Œæˆ",
            "data": result
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 3. åœ¨ main.py ä¸­æ³¨å†Œ

```python
# main.py
from api import news, banner, huawei_dev_news

app = FastAPI(title="NowInOpenHarmony API")

app.include_router(news.router)
app.include_router(banner.router)
app.include_router(huawei_dev_news.router)  # æ–°å¢
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### æ³•å¾‹å’Œé“å¾·

1. **éµå®ˆ robots.txt**: æ£€æŸ¥ç›®æ ‡ç½‘ç«™çš„çˆ¬è™«åè®®
2. **æ§åˆ¶é¢‘ç‡**: ä¸è¦å¯¹ç½‘ç«™é€ æˆå‹åŠ›
3. **å°Šé‡ç‰ˆæƒ**: æ³¨æ˜æ•°æ®æ¥æº
4. **ç”¨æˆ·åè®®**: éµå®ˆç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾

### æŠ€æœ¯é™åˆ¶

1. **åçˆ¬è™«æœºåˆ¶**: æœ‰äº›ç½‘ç«™æœ‰éªŒè¯ç ã€IPé™åˆ¶ç­‰
2. **åŠ¨æ€å†…å®¹**: éœ€è¦ä½¿ç”¨ Selenium çš„æƒ…å†µ
3. **æ•°æ®ç»“æ„**: ç½‘ç«™æ”¹ç‰ˆå¯èƒ½å¯¼è‡´çˆ¬è™«å¤±æ•ˆ
4. **å­—ç¬¦ç¼–ç **: æ³¨æ„å¤„ç†å„ç§ç¼–ç é—®é¢˜

### èµ„æºç®¡ç†

1. **å†…å­˜ä½¿ç”¨**: å¤§é‡æ•°æ®æ—¶æ³¨æ„åˆ†æ‰¹å¤„ç†
2. **ç½‘ç»œè¿æ¥**: ä½¿ç”¨è¿æ¥æ± ,åŠæ—¶å…³é—­è¿æ¥
3. **æ•°æ®åº“è¿æ¥**: é¿å…è¿æ¥æ³„æ¼
4. **ç£ç›˜ç©ºé—´**: å®šæœŸæ¸…ç†æ—§æ•°æ®

## ğŸ“š å‚è€ƒèµ„æ–™

- [Requests æ–‡æ¡£](https://docs.python-requests.org/)
- [BeautifulSoup æ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium æ–‡æ¡£](https://selenium-python.readthedocs.io/)
- [FastAPI æ–‡æ¡£](https://fastapi.tiangolo.com/)
- [robots.txt è§„èŒƒ](https://www.robotstxt.org/)

## ğŸ†˜ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜?

1. æŸ¥çœ‹ `COLLABORATION_GUIDE.md` åä½œæŒ‡å—
2. æŸ¥çœ‹ç°æœ‰çˆ¬è™«ä»£ç ä½œä¸ºå‚è€ƒ
3. åœ¨å›¢é˜Ÿç¾¤é‡Œæé—®
4. è”ç³»æ¶æ„è´Ÿè´£äºº

ç¥å¼€å‘é¡ºåˆ©! ğŸš€
