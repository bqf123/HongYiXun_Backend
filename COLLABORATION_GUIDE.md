# å›¢é˜Ÿåä½œæŒ‡å—

**é¡¹ç›®åç§°**: NowInOpenHarmony Backend
**åä½œåœºæ™¯**: å¤šäººå¹¶è¡Œå¼€å‘çˆ¬è™«æ•°æ®æº
**æ›´æ–°æ—¥æœŸ**: 2025-11-16

---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [å›¢é˜Ÿåˆ†å·¥](#å›¢é˜Ÿåˆ†å·¥)
3. [å¼€å‘æµç¨‹](#å¼€å‘æµç¨‹)
4. [ä»£ç è§„èŒƒ](#ä»£ç è§„èŒƒ)
5. [åˆ†æ”¯ç®¡ç†ç­–ç•¥](#åˆ†æ”¯ç®¡ç†ç­–ç•¥)
6. [æ–‡ä»¶å‘½åè§„èŒƒ](#æ–‡ä»¶å‘½åè§„èŒƒ)
7. [ä»£ç å®¡æŸ¥æµç¨‹](#ä»£ç å®¡æŸ¥æµç¨‹)
8. [å†²çªé¢„é˜²](#å†²çªé¢„é˜²)
9. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## é¡¹ç›®æ¦‚è¿°

### å½“å‰æ¶æ„

```
NowInOpenHarmony Backend
â”œâ”€â”€ æ•°æ®é‡‡é›†å±‚ (services/)
â”‚   â”œâ”€â”€ âœ… OpenHarmony å®˜ç½‘æ–°é—»çˆ¬è™« (å·²å®Œæˆ)
â”‚   â”œâ”€â”€ âœ… OpenHarmony æŠ€æœ¯åšå®¢çˆ¬è™« (å·²å®Œæˆ)
â”‚   â”œâ”€â”€ âœ… OpenHarmony Banner çˆ¬è™« (å·²å®Œæˆ)
â”‚   â”œâ”€â”€ ğŸ†• æ–°æ•°æ®æº A çˆ¬è™« (å¾…å¼€å‘ - æˆå‘˜1)
â”‚   â””â”€â”€ ğŸ†• æ–°æ•°æ®æº B çˆ¬è™« (å¾…å¼€å‘ - æˆå‘˜2)
â”œâ”€â”€ APIå±‚ (api/)
â”œâ”€â”€ æ ¸å¿ƒå±‚ (core/)
â””â”€â”€ æ¨¡å‹å±‚ (models/)
```

### æŠ€æœ¯æ ˆ
- **Webæ¡†æ¶**: FastAPI 0.104.1
- **çˆ¬è™«åº“**: Requests + BeautifulSoup4 + Selenium
- **æ•°æ®åº“**: SQLite (å¼€å‘) / PostgreSQL (ç”Ÿäº§)
- **ç‰ˆæœ¬æ§åˆ¶**: Git + GitHub

---

## å›¢é˜Ÿåˆ†å·¥

### è§’è‰²åˆ†é…

| è§’è‰² | è´Ÿè´£äºº | èŒè´£ | å·¥ä½œå†…å®¹ |
|------|--------|------|---------|
| **æ¶æ„è´Ÿè´£äºº** | ä½  | é¡¹ç›®æ¶æ„ã€ä»£ç å®¡æŸ¥ã€é›†æˆ | ç»´æŠ¤æ ¸å¿ƒæ¨¡å—ã€å®¡æŸ¥PRã€è§£å†³å†²çª |
| **çˆ¬è™«å¼€å‘è€… A** | æˆå‘˜1 | å¼€å‘æ•°æ®æºAçˆ¬è™« | çˆ¬è™«é€»è¾‘ã€APIæ¥å£ã€æ•°æ®æ¨¡å‹ |
| **çˆ¬è™«å¼€å‘è€… B** | æˆå‘˜2 | å¼€å‘æ•°æ®æºBçˆ¬è™« | çˆ¬è™«é€»è¾‘ã€APIæ¥å£ã€æ•°æ®æ¨¡å‹ |

### å·¥ä½œè¾¹ç•Œ

**âœ… å„æˆå‘˜å¯ä»¥ä¿®æ”¹çš„æ–‡ä»¶**:
- `services/your_crawler_name.py` - è‡ªå·±çš„çˆ¬è™«æ–‡ä»¶(æ–°å»º)
- `api/your_api_name.py` - è‡ªå·±çš„APIè·¯ç”±æ–‡ä»¶(æ–°å»º)
- `models/your_model_name.py` - è‡ªå·±çš„æ•°æ®æ¨¡å‹æ–‡ä»¶(æ–°å»º)
- `requirements.txt` - æ·»åŠ è‡ªå·±éœ€è¦çš„ä¾èµ–(è°¨æ…)

**âš ï¸ éœ€è¦åè°ƒä¿®æ”¹çš„æ–‡ä»¶**:
- `main.py` - æ³¨å†ŒAPIè·¯ç”±(éœ€è¦åè°ƒé¿å…å†²çª)
- `core/database.py` - æ·»åŠ æ•°æ®åº“è¡¨(éœ€è¦åè°ƒ)
- `core/scheduler.py` - æ·»åŠ å®šæ—¶ä»»åŠ¡(éœ€è¦åè°ƒ)
- `services/news_service.py` - å¦‚æœéœ€è¦é›†æˆåˆ°ç»Ÿä¸€æœåŠ¡

**âŒ ä¸è¦ä¿®æ”¹çš„æ ¸å¿ƒæ–‡ä»¶**:
- `core/cache.py` - ç¼“å­˜ç®¡ç†(é™¤éç»è¿‡è®¨è®º)
- `core/config.py` - é…ç½®ç®¡ç†(é™¤éç»è¿‡è®¨è®º)
- `core/logging_config.py` - æ—¥å¿—é…ç½®
- å…¶ä»–æˆå‘˜çš„çˆ¬è™«æ–‡ä»¶

---

## å¼€å‘æµç¨‹

### å®Œæ•´å¼€å‘å‘¨æœŸ

```mermaid
graph TD
    A[é¢†å–ä»»åŠ¡] --> B[åˆ›å»ºåŠŸèƒ½åˆ†æ”¯]
    B --> C[æœ¬åœ°å¼€å‘]
    C --> D[è‡ªæµ‹é€šè¿‡]
    D --> E[æäº¤ä»£ç ]
    E --> F[åˆ›å»º Pull Request]
    F --> G[ä»£ç å®¡æŸ¥]
    G --> H{å®¡æŸ¥é€šè¿‡?}
    H -->|æ˜¯| I[åˆå¹¶åˆ°main]
    H -->|å¦| J[ä¿®æ”¹ä»£ç ]
    J --> E
    I --> K[åˆ é™¤åŠŸèƒ½åˆ†æ”¯]
```

### è¯¦ç»†æ­¥éª¤

#### 1ï¸âƒ£ é¢†å–ä»»åŠ¡å¹¶åˆ›å»ºåˆ†æ”¯

```bash
# 1. ç¡®ä¿æœ¬åœ°mainåˆ†æ”¯æ˜¯æœ€æ–°çš„
git checkout main
git pull origin main

# 2. åˆ›å»ºä½ çš„åŠŸèƒ½åˆ†æ”¯(å‘½åè§„èŒƒè§ä¸‹æ–‡)
# æ ¼å¼: feature/<æ•°æ®æºåç§°>-crawler
git checkout -b feature/datasource-a-crawler

# ä¾‹å¦‚:
# æˆå‘˜1: git checkout -b feature/huawei-news-crawler
# æˆå‘˜2: git checkout -b feature/harmonyos-forum-crawler
```

#### 2ï¸âƒ£ æœ¬åœ°å¼€å‘

```bash
# 1. åˆ›å»ºä½ çš„çˆ¬è™«æ–‡ä»¶
# services/<æ•°æ®æºåç§°>_crawler.py

# 2. åˆ›å»ºä½ çš„APIæ–‡ä»¶
# api/<æ•°æ®æºåç§°>.py

# 3. åˆ›å»ºä½ çš„æ¨¡å‹æ–‡ä»¶(å¦‚æœéœ€è¦ç‹¬ç«‹æ¨¡å‹)
# models/<æ•°æ®æºåç§°>.py

# 4. å®šæœŸæäº¤åˆ°æœ¬åœ°
git add .
git commit -m "feat: æ·»åŠ xxxåŠŸèƒ½"

# 5. å®šæœŸæ¨é€åˆ°è¿œç¨‹(å¤‡ä»½ + è®©å›¢é˜ŸçŸ¥é“ä½ çš„è¿›åº¦)
git push origin feature/datasource-a-crawler
```

#### 3ï¸âƒ£ å¼€å‘å®Œæˆåæäº¤

```bash
# 1. ç¡®ä¿ä»£ç ç¬¦åˆè§„èŒƒ(è¿è¡Œæµ‹è¯•)
python -m pytest tests/  # å¦‚æœæœ‰æµ‹è¯•
python your_crawler.py   # æ‰‹åŠ¨æµ‹è¯•

# 2. æ›´æ–° main åˆ†æ”¯(é˜²æ­¢å†²çª)
git checkout main
git pull origin main

# 3. å›åˆ°ä½ çš„åˆ†æ”¯å¹¶åˆå¹¶ main çš„æœ€æ–°æ›´æ”¹
git checkout feature/datasource-a-crawler
git merge main

# 4. è§£å†³å†²çª(å¦‚æœæœ‰)
# ç¼–è¾‘å†²çªæ–‡ä»¶ -> git add . -> git commit

# 5. æ¨é€æœ€ç»ˆç‰ˆæœ¬
git push origin feature/datasource-a-crawler
```

#### 4ï¸âƒ£ åˆ›å»º Pull Request

åœ¨ GitHub ä¸Š:
1. è¿›å…¥ä»“åº“é¡µé¢
2. ç‚¹å‡» "Pull requests" -> "New pull request"
3. é€‰æ‹©: `base: main` <- `compare: feature/datasource-a-crawler`
4. å¡«å†™ PR æè¿°(ä½¿ç”¨ä¸‹é¢çš„æ¨¡æ¿)

**PR æè¿°æ¨¡æ¿**:
```markdown
## ğŸ“ å˜æ›´è¯´æ˜
æ·»åŠ  [æ•°æ®æºåç§°] çˆ¬è™«

## âœ¨ æ–°å¢åŠŸèƒ½
- [ ] çˆ¬è™«é€»è¾‘å®ç° (`services/xxx_crawler.py`)
- [ ] API æ¥å£å®ç° (`api/xxx.py`)
- [ ] æ•°æ®æ¨¡å‹å®šä¹‰ (`models/xxx.py`)
- [ ] æ•°æ®åº“è¡¨åˆ›å»º
- [ ] å®šæ—¶ä»»åŠ¡é…ç½®(å¯é€‰)

## ğŸ§ª æµ‹è¯•æƒ…å†µ
- [x] æœ¬åœ°æµ‹è¯•é€šè¿‡
- [x] çˆ¬è™«èƒ½æ­£å¸¸è·å–æ•°æ®
- [x] API æ¥å£æ­£å¸¸å“åº”
- [ ] å·²æ·»åŠ å•å…ƒæµ‹è¯•(å¯é€‰)

## ğŸ“¸ æµ‹è¯•æˆªå›¾
(å¯é€‰) è´´ä¸Š API æµ‹è¯•æˆªå›¾æˆ–çˆ¬è™«è¿è¡Œæ—¥å¿—

## ğŸ”— ç›¸å…³ Issue
å…³é—­ #issueç¼–å· (å¦‚æœæœ‰)

## âš ï¸ æ³¨æ„äº‹é¡¹
- ä¿®æ”¹äº†å“ªäº›å…±äº«æ–‡ä»¶: `main.py` ç¬¬XXè¡Œ
- æ·»åŠ äº†å“ªäº›ä¾èµ–: `beautifulsoup4==4.12.2`
```

#### 5ï¸âƒ£ ä»£ç å®¡æŸ¥ä¸åˆå¹¶

- **å®¡æŸ¥äºº**: æ¶æ„è´Ÿè´£äºº
- **å®¡æŸ¥æ—¶é—´**: 24å°æ—¶å†…
- **åˆå¹¶**: å®¡æŸ¥é€šè¿‡åç”±æ¶æ„è´Ÿè´£äººåˆå¹¶

---

## ä»£ç è§„èŒƒ

### Python ä»£ç é£æ ¼

éµå¾ª **PEP 8** è§„èŒƒ:

```python
# âœ… å¥½çš„ç¤ºä¾‹
class MyNewsCrawler:
    """æ–°æ•°æ®æºçˆ¬è™«ç±»

    è´Ÿè´£ä» https://example.com çˆ¬å–æ–°é—»æ•°æ®
    """

    def __init__(self, base_url: str):
        self.base_url = base_url
        self.session = requests.Session()

    def fetch_news(self, page: int = 1) -> List[Dict]:
        """è·å–æ–°é—»åˆ—è¡¨

        Args:
            page: é¡µç ï¼Œä»1å¼€å§‹

        Returns:
            æ–°é—»åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«æ ‡é¢˜ã€é“¾æ¥ã€æ—¥æœŸç­‰
        """
        pass

# âŒ ä¸å¥½çš„ç¤ºä¾‹
class mynewscrawler:  # ç±»ååº”è¯¥ç”¨å¤§é©¼å³°
    def FetchNews(self,page):  # æ–¹æ³•ååº”è¯¥ç”¨å°å†™+ä¸‹åˆ’çº¿
        url=self.base_url+'/news?page='+str(page)  # å­—ç¬¦ä¸²æ‹¼æ¥åº”è¯¥ç”¨f-string
        return requests.get(url).json()
```

### çˆ¬è™«å¼€å‘è§„èŒƒ

**1. æ–‡ä»¶ç»“æ„æ¨¡æ¿**

```python
# services/your_datasource_crawler.py
"""
[æ•°æ®æºåç§°] çˆ¬è™«
ç›®æ ‡ç½‘ç«™: https://example.com
æ•°æ®ç±»å‹: æ–°é—»/è®ºå›/æ–‡æ¡£ç­‰
"""

import logging
from typing import List, Dict, Optional
from datetime import datetime

import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class YourDataSourceCrawler:
    """[æ•°æ®æºåç§°]çˆ¬è™«ç±»"""

    def __init__(self, base_url: str = "https://example.com"):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            base_url: ç›®æ ‡ç½‘ç«™æ ¹URL
        """
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def fetch_list(self, page: int = 1) -> List[Dict]:
        """
        è·å–åˆ—è¡¨é¡µæ•°æ®

        Args:
            page: é¡µç 

        Returns:
            æ•°æ®åˆ—è¡¨
        """
        try:
            url = f"{self.base_url}/list?page={page}"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            # è§£æé€»è¾‘...

            return []

        except Exception as e:
            logger.error(f"è·å–åˆ—è¡¨é¡µå¤±è´¥: {e}")
            return []

    def fetch_detail(self, detail_url: str) -> Optional[Dict]:
        """
        è·å–è¯¦æƒ…é¡µæ•°æ®

        Args:
            detail_url: è¯¦æƒ…é¡µURL

        Returns:
            è¯¦æƒ…æ•°æ®å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            response = self.session.get(detail_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            # è§£æé€»è¾‘...

            return {}

        except Exception as e:
            logger.error(f"è·å–è¯¦æƒ…é¡µå¤±è´¥ {detail_url}: {e}")
            return None

    def save_to_database(self, data: List[Dict]) -> int:
        """
        ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®åˆ—è¡¨

        Returns:
            æˆåŠŸä¿å­˜çš„æ•°æ®æ¡æ•°
        """
        from core.database import execute_update

        count = 0
        for item in data:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                # æ’å…¥æ•°æ®åº“
                count += 1
            except Exception as e:
                logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")

        return count


# æ¨¡å—çº§å‡½æ•° - ä¾›å¤–éƒ¨è°ƒç”¨
def crawl_all() -> Dict[str, any]:
    """
    æ‰§è¡Œå®Œæ•´çˆ¬å–æµç¨‹

    Returns:
        åŒ…å«çˆ¬å–ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    crawler = YourDataSourceCrawler()

    all_data = []
    for page in range(1, 6):  # çˆ¬å–å‰5é¡µ
        data = crawler.fetch_list(page)
        all_data.extend(data)

    saved_count = crawler.save_to_database(all_data)

    return {
        "total": len(all_data),
        "saved": saved_count,
        "status": "success"
    }


if __name__ == "__main__":
    # æœ¬åœ°æµ‹è¯•ä»£ç 
    result = crawl_all()
    print(f"çˆ¬å–å®Œæˆ: {result}")
```

**2. API å¼€å‘æ¨¡æ¿**

```python
# api/your_datasource.py
"""
[æ•°æ®æºåç§°] APIè·¯ç”±
"""

from typing import Optional
from fastapi import APIRouter, Query, HTTPException

from models.your_model import YourDataModel, YourDataResponse
from services.your_datasource_crawler import YourDataSourceCrawler

router = APIRouter(prefix="/api/your-datasource", tags=["æ‚¨çš„æ•°æ®æº"])


@router.get("/", response_model=YourDataResponse)
async def get_data_list(
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    keyword: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯")
):
    """
    è·å–æ•°æ®åˆ—è¡¨
    """
    try:
        # ä»æ•°æ®åº“æŸ¥è¯¢æ•°æ®
        # ...

        return {
            "code": 200,
            "message": "success",
            "data": {
                "total": 100,
                "page": page,
                "page_size": page_size,
                "items": []
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/crawl")
async def trigger_crawl():
    """
    æ‰‹åŠ¨è§¦å‘çˆ¬è™«
    """
    from services.your_datasource_crawler import crawl_all

    try:
        result = crawl_all()
        return {
            "code": 200,
            "message": "çˆ¬å–å®Œæˆ",
            "data": result
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/status")
async def get_status():
    """
    è·å–æœåŠ¡çŠ¶æ€
    """
    return {
        "code": 200,
        "message": "success",
        "data": {
            "status": "running",
            "last_update": "2025-11-16 10:00:00"
        }
    }
```

**3. æ•°æ®æ¨¡å‹æ¨¡æ¿**

```python
# models/your_model.py
"""
[æ•°æ®æºåç§°] æ•°æ®æ¨¡å‹
"""

from typing import Optional, List
from datetime import datetime
from pydantic import BaseModel, Field


class YourDataModel(BaseModel):
    """å•æ¡æ•°æ®æ¨¡å‹"""

    id: int = Field(..., description="æ•°æ®ID")
    title: str = Field(..., description="æ ‡é¢˜")
    url: str = Field(..., description="é“¾æ¥")
    content: Optional[str] = Field(None, description="å†…å®¹")
    author: Optional[str] = Field(None, description="ä½œè€…")
    publish_time: Optional[datetime] = Field(None, description="å‘å¸ƒæ—¶é—´")
    created_at: datetime = Field(default_factory=datetime.now, description="åˆ›å»ºæ—¶é—´")

    class Config:
        json_schema_extra = {
            "example": {
                "id": 1,
                "title": "ç¤ºä¾‹æ ‡é¢˜",
                "url": "https://example.com/article/1",
                "content": "æ–‡ç« å†…å®¹...",
                "author": "å¼ ä¸‰",
                "publish_time": "2025-11-16T10:00:00",
                "created_at": "2025-11-16T10:05:00"
            }
        }


class YourDataResponse(BaseModel):
    """APIå“åº”æ¨¡å‹"""

    code: int = Field(200, description="çŠ¶æ€ç ")
    message: str = Field("success", description="å“åº”æ¶ˆæ¯")
    data: dict = Field(..., description="å“åº”æ•°æ®")
```

### æ•°æ®åº“è§„èŒƒ

**è¡¨å‘½åè§„èŒƒ**:
```python
# è¡¨åæ ¼å¼: <æ•°æ®æºåç§°>_<æ•°æ®ç±»å‹>
# ä¾‹å¦‚:
your_datasource_articles  # ä½ çš„æ•°æ®æºçš„æ–‡ç« è¡¨
your_datasource_comments  # ä½ çš„æ•°æ®æºçš„è¯„è®ºè¡¨
```

**å»ºè¡¨è¯­å¥ç¤ºä¾‹**:
```python
# åœ¨ services/your_datasource_crawler.py çš„ __init__ æˆ–ä¸“é—¨çš„åˆå§‹åŒ–å‡½æ•°ä¸­

from core.database import execute_update

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""

    create_table_sql = """
    CREATE TABLE IF NOT EXISTS your_datasource_articles (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        title TEXT NOT NULL,
        url TEXT UNIQUE NOT NULL,
        content TEXT,
        author TEXT,
        publish_time DATETIME,
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
        updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
    )
    """

    execute_update(create_table_sql)

    # åˆ›å»ºç´¢å¼•
    execute_update("CREATE INDEX IF NOT EXISTS idx_url ON your_datasource_articles(url)")
    execute_update("CREATE INDEX IF NOT EXISTS idx_publish_time ON your_datasource_articles(publish_time)")
```

---

## åˆ†æ”¯ç®¡ç†ç­–ç•¥

### åˆ†æ”¯ç±»å‹

```
main (ä¸»åˆ†æ”¯)
  â”œâ”€â”€ feature/datasource-a-crawler (åŠŸèƒ½åˆ†æ”¯ - æˆå‘˜1)
  â”œâ”€â”€ feature/datasource-b-crawler (åŠŸèƒ½åˆ†æ”¯ - æˆå‘˜2)
  â”œâ”€â”€ hotfix/fix-cors-issue (çƒ­ä¿®å¤åˆ†æ”¯)
  â””â”€â”€ release/v1.0.0 (å‘å¸ƒåˆ†æ”¯ - å¯é€‰)
```

### åˆ†æ”¯å‘½åè§„èŒƒ

| åˆ†æ”¯ç±»å‹ | å‘½åæ ¼å¼ | ç¤ºä¾‹ | è¯´æ˜ |
|---------|---------|------|------|
| åŠŸèƒ½åˆ†æ”¯ | `feature/<åŠŸèƒ½åç§°>` | `feature/huawei-news-crawler` | å¼€å‘æ–°åŠŸèƒ½ |
| ä¿®å¤åˆ†æ”¯ | `bugfix/<é—®é¢˜æè¿°>` | `bugfix/fix-encoding-error` | ä¿®å¤bug |
| çƒ­ä¿®å¤ | `hotfix/<ç´§æ€¥é—®é¢˜>` | `hotfix/fix-cors-issue` | ç´§æ€¥ä¿®å¤ç”Ÿäº§é—®é¢˜ |
| å‘å¸ƒåˆ†æ”¯ | `release/v<ç‰ˆæœ¬å·>` | `release/v1.0.0` | å‡†å¤‡å‘å¸ƒç‰ˆæœ¬ |

### åˆ†æ”¯ç”Ÿå‘½å‘¨æœŸ

```bash
# 1. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
git checkout -b feature/your-crawler main

# 2. å¼€å‘è¿‡ç¨‹ä¸­
# - å®šæœŸ commit
# - å®šæœŸ push åˆ°è¿œç¨‹
# - å®šæœŸä» main åˆå¹¶æœ€æ–°ä»£ç 

# 3. å®Œæˆå¼€å‘
# - åˆ›å»º Pull Request
# - ä»£ç å®¡æŸ¥
# - åˆå¹¶åˆ° main

# 4. æ¸…ç†åˆ†æ”¯
git branch -d feature/your-crawler  # åˆ é™¤æœ¬åœ°åˆ†æ”¯
git push origin --delete feature/your-crawler  # åˆ é™¤è¿œç¨‹åˆ†æ”¯
```

---

## æ–‡ä»¶å‘½åè§„èŒƒ

### ç›®å½•ç»“æ„

```
services/
â”œâ”€â”€ openharmony_news_crawler.py        âœ… å·²å­˜åœ¨
â”œâ”€â”€ openharmony_blog_crawler.py        âœ… å·²å­˜åœ¨
â”œâ”€â”€ mobile_banner_crawler.py           âœ… å·²å­˜åœ¨
â”œâ”€â”€ your_datasource_a_crawler.py       ğŸ†• æˆå‘˜1åˆ›å»º
â””â”€â”€ your_datasource_b_crawler.py       ğŸ†• æˆå‘˜2åˆ›å»º

api/
â”œâ”€â”€ news.py                            âœ… å·²å­˜åœ¨
â”œâ”€â”€ banner.py                          âœ… å·²å­˜åœ¨
â”œâ”€â”€ your_datasource_a.py               ğŸ†• æˆå‘˜1åˆ›å»º
â””â”€â”€ your_datasource_b.py               ğŸ†• æˆå‘˜2åˆ›å»º

models/
â”œâ”€â”€ news.py                            âœ… å·²å­˜åœ¨
â”œâ”€â”€ banner.py                          âœ… å·²å­˜åœ¨
â”œâ”€â”€ your_datasource_a.py               ğŸ†• æˆå‘˜1åˆ›å»º (å¯é€‰)
â””â”€â”€ your_datasource_b.py               ğŸ†• æˆå‘˜2åˆ›å»º (å¯é€‰)
```

### å‘½åè§„èŒƒ

**Python æ–‡ä»¶å**: å°å†™ + ä¸‹åˆ’çº¿
```
âœ… huawei_news_crawler.py
âœ… harmonyos_forum_crawler.py
âŒ HuaweiNewsCrawler.py
âŒ huawei-news-crawler.py
```

**ç±»å**: å¤§é©¼å³°å‘½å
```python
âœ… class HuaweiNewsCrawler:
âœ… class HarmonyOSForumCrawler:
âŒ class huawei_news_crawler:
âŒ class Huawei_News_Crawler:
```

**å‡½æ•°å/æ–¹æ³•å**: å°å†™ + ä¸‹åˆ’çº¿
```python
âœ… def fetch_news_list():
âœ… def parse_article_detail():
âŒ def FetchNewsList():
âŒ def parseArticleDetail():
```

**å¸¸é‡**: å…¨å¤§å†™ + ä¸‹åˆ’çº¿
```python
âœ… BASE_URL = "https://example.com"
âœ… MAX_RETRY_COUNT = 3
âŒ baseUrl = "https://example.com"
```

---

## å†²çªé¢„é˜²

### å¯èƒ½äº§ç”Ÿå†²çªçš„åœºæ™¯

#### åœºæ™¯1: åŒæ—¶ä¿®æ”¹ `main.py`

**é—®é¢˜**: ä¸¤ä¸ªäººéƒ½è¦åœ¨ `main.py` ä¸­æ³¨å†Œè‡ªå·±çš„è·¯ç”±

**è§£å†³æ–¹æ¡ˆ**:
```python
# main.py - åœ¨æ–‡ä»¶æœ«å°¾é¢„ç•™æ³¨å†ŒåŒºåŸŸ

from fastapi import FastAPI
from api import news, banner

app = FastAPI(title="NowInOpenHarmony API")

# ============= æ ¸å¿ƒè·¯ç”± (ä¸è¦ä¿®æ”¹) =============
app.include_router(news.router)
app.include_router(banner.router)

# ============= æ–°å¢è·¯ç”±åŒºåŸŸ =============
# æˆå‘˜1: åœ¨è¿™é‡Œæ³¨å†Œä½ çš„è·¯ç”±
# from api import your_datasource_a
# app.include_router(your_datasource_a.router)

# æˆå‘˜2: åœ¨è¿™é‡Œæ³¨å†Œä½ çš„è·¯ç”±
# from api import your_datasource_b
# app.include_router(your_datasource_b.router)

# ============= è·¯ç”±æ³¨å†Œç»“æŸ =============
```

**åä½œæµç¨‹**:
1. æˆå‘˜1å®Œæˆå¼€å‘å,å–æ¶ˆæ³¨é‡Šè‡ªå·±çš„è·¯ç”±,æäº¤PR
2. PRåˆå¹¶åˆ°mainå,æˆå‘˜2 pull æœ€æ–°ä»£ç 
3. æˆå‘˜2åœ¨æœ€æ–°ä»£ç åŸºç¡€ä¸Šå–æ¶ˆæ³¨é‡Šè‡ªå·±çš„è·¯ç”±,æäº¤PR

#### åœºæ™¯2: åŒæ—¶ä¿®æ”¹ `requirements.txt`

**é—®é¢˜**: ä¸¤ä¸ªäººéƒ½æ·»åŠ äº†ä¾èµ–,å¯¼è‡´å†²çª

**è§£å†³æ–¹æ¡ˆ**:

1. **å¼€å‘é˜¶æ®µ**: å°†ä¾èµ–å†™åœ¨ä½ çš„PRæè¿°ä¸­
```markdown
## æ–°å¢ä¾èµ–
- selenium==4.15.0
- pillow==10.0.0
```

2. **åˆå¹¶é˜¶æ®µ**: æ¶æ„è´Ÿè´£äººç»Ÿä¸€æ·»åŠ ä¾èµ–

3. **è‡ªåŠ¨åŒ–æ–¹æ¡ˆ** (æ¨è):
```bash
# æ¯ä¸ªæˆå‘˜åœ¨è‡ªå·±çš„åˆ†æ”¯ç»´æŠ¤ä¾èµ–
# åˆ›å»º requirements-<your-name>.txt
echo "selenium==4.15.0" > requirements-member1.txt
echo "pillow==10.0.0" > requirements-member2.txt

# æ¶æ„è´Ÿè´£äººåˆå¹¶æ—¶æ•´åˆ
cat requirements.txt requirements-member1.txt requirements-member2.txt | sort | uniq > requirements-new.txt
mv requirements-new.txt requirements.txt
```

#### åœºæ™¯3: åŒæ—¶ä¿®æ”¹ `core/database.py`

**é—®é¢˜**: ä¸¤ä¸ªäººéƒ½éœ€è¦æ·»åŠ æ•°æ®åº“åˆå§‹åŒ–ä»£ç 

**è§£å†³æ–¹æ¡ˆ**: **ä¸è¦ä¿®æ”¹æ ¸å¿ƒæ•°æ®åº“æ–‡ä»¶**

åœ¨è‡ªå·±çš„çˆ¬è™«æ–‡ä»¶ä¸­ç®¡ç†è‡ªå·±çš„è¡¨:
```python
# services/your_crawler.py

from core.database import execute_update

def init_my_tables():
    """åˆå§‹åŒ–æˆ‘çš„æ•°æ®è¡¨"""
    execute_update("""
        CREATE TABLE IF NOT EXISTS my_datasource_articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            ...
        )
    """)

# åœ¨çˆ¬è™«åˆå§‹åŒ–æ—¶è°ƒç”¨
class YourCrawler:
    def __init__(self):
        init_my_tables()  # åˆå§‹åŒ–è¡¨
        ...
```

#### åœºæ™¯4: åŒæ—¶ä¿®æ”¹ `core/scheduler.py`

**é—®é¢˜**: ä¸¤ä¸ªäººéƒ½è¦æ·»åŠ å®šæ—¶ä»»åŠ¡

**è§£å†³æ–¹æ¡ˆ**:

**æ–¹æ¡ˆA**: æ¯ä¸ªäººåœ¨è‡ªå·±çš„çˆ¬è™«æ–‡ä»¶ä¸­å®ç°å®šæ—¶ä»»åŠ¡ (æ¨è)
```python
# services/your_crawler.py

from apscheduler.schedulers.background import BackgroundScheduler

class YourCrawler:
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self._setup_scheduler()

    def _setup_scheduler(self):
        """é…ç½®å®šæ—¶ä»»åŠ¡"""
        # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
        self.scheduler.add_job(
            self.crawl_all,
            'interval',
            hours=1,
            id='your_crawler_job'
        )
        self.scheduler.start()
```

**æ–¹æ¡ˆB**: åœ¨PRä¸­è¯´æ˜éœ€è¦æ·»åŠ çš„å®šæ—¶ä»»åŠ¡,ç”±æ¶æ„è´Ÿè´£äººç»Ÿä¸€æ·»åŠ åˆ° `core/scheduler.py`

### å†²çªè§£å†³æµç¨‹

å¦‚æœç¡®å®å‘ç”Ÿäº†å†²çª:

```bash
# 1. æ‹‰å–æœ€æ–°çš„ main åˆ†æ”¯
git checkout main
git pull origin main

# 2. å›åˆ°ä½ çš„åŠŸèƒ½åˆ†æ”¯
git checkout feature/your-crawler

# 3. åˆå¹¶ main åˆ°ä½ çš„åˆ†æ”¯
git merge main

# 4. æŸ¥çœ‹å†²çªæ–‡ä»¶
git status

# 5. æ‰‹åŠ¨è§£å†³å†²çª
# ç¼–è¾‘å†²çªæ–‡ä»¶,ä¿ç•™åŒæ–¹çš„ä¿®æ”¹
# åˆ é™¤ <<<<<<<, =======, >>>>>>> æ ‡è®°

# 6. æ ‡è®°å†²çªå·²è§£å†³
git add <å†²çªæ–‡ä»¶>

# 7. å®Œæˆåˆå¹¶
git commit -m "merge: è§£å†³ä¸mainåˆ†æ”¯çš„å†²çª"

# 8. æ¨é€
git push origin feature/your-crawler
```

**å†²çªç¤ºä¾‹**:
```python
# main.py å†²çªç¤ºä¾‹
<<<<<<< HEAD
# æˆå‘˜1çš„ä»£ç 
from api import datasource_a
app.include_router(datasource_a.router)
=======
# æˆå‘˜2çš„ä»£ç 
from api import datasource_b
app.include_router(datasource_b.router)
>>>>>>> main

# è§£å†³ååº”è¯¥æ˜¯:
from api import datasource_a, datasource_b
app.include_router(datasource_a.router)
app.include_router(datasource_b.router)
```

---

## ä»£ç å®¡æŸ¥æµç¨‹

### Pull Request æ£€æŸ¥æ¸…å•

**æ¶æ„è´Ÿè´£äººå®¡æŸ¥æ—¶åº”æ£€æŸ¥**:

#### âœ… ä»£ç è´¨é‡
- [ ] ä»£ç ç¬¦åˆ PEP 8 è§„èŒƒ
- [ ] æœ‰é€‚å½“çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²
- [ ] æ²¡æœ‰ç¡¬ç¼–ç çš„æ•æ„Ÿä¿¡æ¯(å¯†ç ã€APIå¯†é’¥ç­‰)
- [ ] å¼‚å¸¸å¤„ç†å®Œå–„
- [ ] æ—¥å¿—è®°å½•åˆç†

#### âœ… åŠŸèƒ½å®Œæ•´æ€§
- [ ] çˆ¬è™«èƒ½æ­£å¸¸è¿è¡Œ
- [ ] API æ¥å£æ­£å¸¸å“åº”
- [ ] æ•°æ®èƒ½æ­£ç¡®ä¿å­˜åˆ°æ•°æ®åº“
- [ ] é”™è¯¯æƒ…å†µèƒ½æ­£ç¡®å¤„ç†

#### âœ… é›†æˆå…¼å®¹æ€§
- [ ] ä¸å½±å“ç°æœ‰åŠŸèƒ½
- [ ] ä¾èµ–é¡¹æ²¡æœ‰å†²çª
- [ ] æ•°æ®åº“è¡¨åä¸å†²çª
- [ ] APIè·¯ç”±ä¸å†²çª

#### âœ… æ–‡æ¡£å®Œå–„
- [ ] PRæè¿°æ¸…æ™°
- [ ] ä»£ç æœ‰æ³¨é‡Š
- [ ] å¦‚æœ‰æ–°ä¾èµ–,å·²è¯´æ˜ç”¨é€”

### å®¡æŸ¥åé¦ˆ

**ä½¿ç”¨ GitHub Review åŠŸèƒ½**:
```
âœ… Approve - æ‰¹å‡†åˆå¹¶
ğŸ’¬ Comment - æå‡ºå»ºè®®ä½†ä¸é˜»æ­¢åˆå¹¶
âŒ Request changes - å¿…é¡»ä¿®æ”¹åæ‰èƒ½åˆå¹¶
```

**å¸¸è§å®¡æŸ¥æ„è§**:
```markdown
ğŸ“ ä»£ç é£æ ¼å»ºè®®:
- å»ºè®®ä½¿ç”¨ f-string ä»£æ›¿å­—ç¬¦ä¸²æ‹¼æ¥
- å‡½æ•°ååº”è¯¥ç”¨å°å†™+ä¸‹åˆ’çº¿

ğŸ› æ½œåœ¨é—®é¢˜:
- ç¬¬45è¡Œå¯èƒ½ä¼šæŠ›å‡º KeyError,å»ºè®®ä½¿ç”¨ .get() æ–¹æ³•
- ç¼ºå°‘å¼‚å¸¸å¤„ç†,å»ºè®®æ·»åŠ  try-except

ğŸ”§ æ”¹è¿›å»ºè®®:
- å¯ä»¥è€ƒè™‘æ·»åŠ é‡è¯•æœºåˆ¶
- å»ºè®®æå–é­”æ³•æ•°å­—ä¸ºå¸¸é‡

âœ… ä¼˜ç‚¹:
- ä»£ç ç»“æ„æ¸…æ™°
- é”™è¯¯å¤„ç†å®Œå–„
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é¿å…ä¸¤ä¸ªäººåŒæ—¶ä¿®æ”¹åŒä¸€ä¸ªæ–‡ä»¶?

**A**:
1. **æ²Ÿé€šç¬¬ä¸€**: åœ¨å¼€å‘å‰åœ¨å›¢é˜Ÿç¾¤é‡Œè¯´æ˜ä½ è¦ä¿®æ”¹å“ªäº›æ–‡ä»¶
2. **æœ€å°åŒ–ä¿®æ”¹**: å°½é‡åªä¿®æ”¹è‡ªå·±çš„æ–‡ä»¶,ä¸ä¿®æ”¹æ ¸å¿ƒæ–‡ä»¶
3. **é¢‘ç¹åŒæ­¥**: æ¯å¤©å¼€å§‹å·¥ä½œå‰ `git pull` æ‹‰å–æœ€æ–°ä»£ç 

### Q2: æˆ‘çš„åˆ†æ”¯è½åmainå¾ˆå¤šç‰ˆæœ¬,æ€ä¹ˆåŠ?

**A**:
```bash
# å®šæœŸåˆå¹¶mainåˆ°ä½ çš„åˆ†æ”¯
git checkout feature/your-crawler
git merge main

# æˆ–è€…ä½¿ç”¨ rebase (è®©æäº¤å†å²æ›´æ¸…æ™°)
git checkout feature/your-crawler
git rebase main
```

### Q3: æˆ‘ä¸å°å¿ƒåœ¨mainåˆ†æ”¯ä¸Šå¼€å‘äº†,æ€ä¹ˆåŠ?

**A**:
```bash
# 1. åˆ›å»ºæ–°åˆ†æ”¯(ä¿ç•™ä½ çš„ä¿®æ”¹)
git checkout -b feature/your-crawler

# 2. æ¨é€æ–°åˆ†æ”¯
git push origin feature/your-crawler

# 3. åˆ‡å›mainå¹¶é‡ç½®
git checkout main
git reset --hard origin/main
```

### Q4: å¦‚ä½•æµ‹è¯•åˆ«äººçš„ä»£ç ?

**A**:
```bash
# 1. æ‹‰å–å¯¹æ–¹çš„åˆ†æ”¯
git fetch origin
git checkout feature/member-a-crawler

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. è¿è¡Œæµ‹è¯•
python services/member_a_crawler.py

# 4. æµ‹è¯•API
uvicorn main:app --reload
# è®¿é—® http://localhost:8001/docs
```

### Q5: ä»£ç åˆå¹¶åå‘ç°é—®é¢˜,å¦‚ä½•å›æ»š?

**A**:
```bash
# 1. æŸ¥çœ‹æäº¤å†å²
git log --oneline

# 2. å›æ»šåˆ°æŒ‡å®šæäº¤
git revert <commit-hash>

# 3. æ¨é€å›æ»š
git push origin main
```

### Q6: å¦‚ä½•æŸ¥çœ‹é¡¹ç›®å½“å‰çš„å¼€å‘è¿›åº¦?

**A**:
- æŸ¥çœ‹ GitHub çš„ "Branches" é¡µé¢,çœ‹æœ‰å“ªäº›æ´»è·ƒåˆ†æ”¯
- æŸ¥çœ‹ "Pull Requests" é¡µé¢,çœ‹æœ‰å“ªäº›å¾…å®¡æŸ¥çš„PR
- å›¢é˜Ÿå®šæœŸåœ¨ç¾¤é‡ŒåŒæ­¥è¿›åº¦

### Q7: ä¾èµ–é¡¹å®‰è£…å¤±è´¥æ€ä¹ˆåŠ?

**A**:
```bash
# 1. å‡çº§ pip
python -m pip install --upgrade pip

# 2. ä½¿ç”¨å›½å†…é•œåƒæº
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# 3. å¦‚æœæŸä¸ªåŒ…å®‰è£…å¤±è´¥,å…ˆè·³è¿‡
pip install -r requirements.txt --no-deps

# 4. å•ç‹¬å®‰è£…é—®é¢˜åŒ…
pip install <package-name>
```

---

## å¼€å‘æ£€æŸ¥æ¸…å•

### å¼€å§‹å¼€å‘å‰

- [ ] å·²é˜…è¯»æœ¬åä½œæŒ‡å—
- [ ] å·²å…‹éš†ä»“åº“åˆ°æœ¬åœ°
- [ ] å·²åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
- [ ] å·²äº†è§£ç°æœ‰ä»£ç ç»“æ„
- [ ] å·²æ˜ç¡®è‡ªå·±çš„æ•°æ®æºå’Œä»»åŠ¡

### å¼€å‘è¿‡ç¨‹ä¸­

- [ ] å®šæœŸ commit (æ¯å®Œæˆä¸€ä¸ªå°åŠŸèƒ½å°±æäº¤)
- [ ] å®šæœŸ push (æ¯å¤©è‡³å°‘æ¨é€ä¸€æ¬¡)
- [ ] å®šæœŸä» main åˆå¹¶æœ€æ–°ä»£ç 
- [ ] é‡åˆ°é—®é¢˜åŠæ—¶åœ¨ç¾¤é‡Œæ²Ÿé€š
- [ ] ä¿æŒä»£ç é£æ ¼ä¸€è‡´

### æäº¤PRå‰

- [ ] ä»£ç å·²æœ¬åœ°æµ‹è¯•é€šè¿‡
- [ ] å·²ä» main åˆå¹¶æœ€æ–°ä»£ç 
- [ ] å·²è§£å†³æ‰€æœ‰å†²çª
- [ ] å·²æ·»åŠ å¿…è¦çš„æ³¨é‡Š
- [ ] å·²å¡«å†™å®Œæ•´çš„PRæè¿°
- [ ] å·²æ£€æŸ¥æ²¡æœ‰æäº¤æ•æ„Ÿä¿¡æ¯

### PRåˆå¹¶å

- [ ] åˆ é™¤æœ¬åœ°åŠŸèƒ½åˆ†æ”¯
- [ ] åˆ é™¤è¿œç¨‹åŠŸèƒ½åˆ†æ”¯
- [ ] æ‹‰å–æœ€æ–°çš„ main åˆ†æ”¯
- [ ] åº†ç¥ä¸€ä¸‹ ğŸ‰

---

## ç´§æ€¥è”ç³»æ–¹å¼

| è§’è‰² | å§“å | è”ç³»æ–¹å¼ | å¯ç”¨æ—¶é—´ |
|------|------|---------|---------|
| æ¶æ„è´Ÿè´£äºº | [ä½ çš„åå­—] | [å¾®ä¿¡/é‚®ç®±] | å·¥ä½œæ—¥ 9:00-18:00 |
| æˆå‘˜1 | [æˆå‘˜1] | [å¾®ä¿¡/é‚®ç®±] | [æ—¶é—´] |
| æˆå‘˜2 | [æˆå‘˜2] | [å¾®ä¿¡/é‚®ç®±] | [æ—¶é—´] |

**å›¢é˜Ÿæ²Ÿé€šæ¸ é“**:
- å¾®ä¿¡ç¾¤: [ç¾¤åç§°]
- é¡¹ç›®çœ‹æ¿: [Trello/Notioné“¾æ¥]
- ä»£ç ä»“åº“: https://github.com/XBXyftx/HongYiXun_Backend

---

## é™„å½•

### A. å¿«é€Ÿå‚è€ƒå‘½ä»¤

```bash
# Git å¸¸ç”¨å‘½ä»¤
git status                              # æŸ¥çœ‹çŠ¶æ€
git add .                               # æ·»åŠ æ‰€æœ‰ä¿®æ”¹
git commit -m "message"                 # æäº¤
git push origin <branch>                # æ¨é€
git pull origin main                    # æ‹‰å–main
git checkout -b <branch>                # åˆ›å»ºå¹¶åˆ‡æ¢åˆ†æ”¯
git merge main                          # åˆå¹¶mainåˆ°å½“å‰åˆ†æ”¯
git log --oneline                       # æŸ¥çœ‹æäº¤å†å²

# é¡¹ç›®è¿è¡Œ
python run.py                           # å¯åŠ¨æœåŠ¡
python services/your_crawler.py         # æµ‹è¯•çˆ¬è™«
pip install -r requirements.txt         # å®‰è£…ä¾èµ–
```

### B. æ¨èå·¥å…·

- **ä»£ç ç¼–è¾‘å™¨**: VS Code / PyCharm
- **Git GUI**: GitHub Desktop / SourceTree
- **API æµ‹è¯•**: Postman / curl
- **æ•°æ®åº“æŸ¥çœ‹**: DB Browser for SQLite
- **Python ç¯å¢ƒ**: venv / conda

### C. å­¦ä¹ èµ„æº

- Gitæ•™ç¨‹: https://www.liaoxuefeng.com/wiki/896043488029600
- Pythonçˆ¬è™«æ•™ç¨‹: https://docs.python-requests.org/
- FastAPIæ–‡æ¡£: https://fastapi.tiangolo.com/zh/

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-11-16
**ç»´æŠ¤è€…**: [ä½ çš„åå­—]

å¦‚æœ‰ç–‘é—®,è¯·éšæ—¶åœ¨å›¢é˜Ÿç¾¤é‡Œæé—®æˆ–è”ç³»æ¶æ„è´Ÿè´£äºº! ğŸ’ª
